---
title: "How to generate a robots.txt file in React Router 7 and Remix"
description: "Learn how to dynamically generate a robots.txt file for your React Router 7 or Remix app with environment-based rules and automatic sitemap integration."
publicationDate: "2025-07-21"
modificationDate: "2025-07-21"
tags: ["React Router 7", "Remix", "SEO", "robots.txt"]
---

import Alert from "@/components/Alert.astro";
import ProfileBadge from "@/components/ProfileBadge.astro";
import NewsletterForm from "@/components/react/NewsletterForm";

A properly configured `robots.txt` file is crucial for controlling how search engines crawl and index your website. Instead of serving a static file, generating it dynamically allows you to adapt the rules based on your environment.

<Alert type="question" title="What exactly is a robots.txt file?">
  A `robots.txt` file tells search engines which parts of your site they’re
  allowed to visit and which parts to skip. It helps control crawler behavior to
  avoid unnecessary load or indexing of unimportant routes. **While it doesn’t
  guarantee pages stay out of search results**, it’s useful for guiding how bots
  interact with your app.
</Alert>

## Install the `@forge42/seo-tools` package

If you haven't already installed it for your [sitemap generation](/blog/sitemap-react-router-7), add the <ProfileBadge platform="GitHub" handle="forge-42/seo-tools">@forge42/seo-tools</ProfileBadge> package. It provides utilities for handling SEO-related tasks and has a dedicated `@forge42/seo-tools/remix` export that works with Remix and React Router 7 applications.

```shellscript
npm install @forge42/seo-tools
```

## Create the `robots.txt` resource route

Create a file at `app/routes/robots.txt.ts`. This will serve as a [resource route](https://reactrouter.com/how-to/resource-routes) for dynamically generating your robots.txt.

```typescript filename="app/routes/robots.txt.ts" showLineNumbers
import { generateRobotsTxt } from "@forge42/seo-tools/robots";
import { href } from "react-router";
import type { Route } from "./+types/robots.txt";

export async function loader({ request }: Route.LoaderArgs) {
  const isProductionDeployment = process.env.VERCEL_ENV === "production";
  const { origin } = new URL(request.url);

  const robotsTxt = generateRobotsTxt([
    {
      userAgent: "*",
      [isProductionDeployment ? "allow" : "disallow"]: ["/"],
      ...(isProductionDeployment
        ? {
            disallow: ["/api/"],
          }
        : {}),
      sitemap: [origin + href("/sitemap.xml")],
    },
  ]);

  return new Response(robotsTxt, {
    headers: {
      "Content-Type": "text/plain",
    },
  });
}
```

### Breaking Down the Implementation

<NewsletterForm />

Let's examine what each part of this code does:

#### Environment-Based Access Control

```typescript showLineNumbers=5
const isProductionDeployment = process.env.VERCEL_ENV === "production";
```

I am using Vercel as my hosting service and here as an example, but you can adapt this check based on your deployment platform:

- **Vercel**: `process.env.VERCEL_ENV === "production"`
- **Netlify**: `process.env.CONTEXT === "production"`
- **Generic**: `process.env.NODE_ENV === "production"`

#### Dynamic Allow/Disallow Rules

```typescript showLineNumbers=11
[isProductionDeployment ? "allow" : "disallow"]: ["/"],
```

This pattern uses computed property names to either:

- **Production**: Allow all routes (`allow: ["/"]`)
- **Non-production**: Block all routes (`disallow: ["/"]`)

<Alert type="tip">
  Blocking staging, preview, or development environments should prevent search
  engines from indexing incomplete or test content that could hurt your SEO.
</Alert>

#### Production-Specific Rules

```typescript showLineNumbers=12
...(isProductionDeployment
  ? {
      disallow: ["/api/"],
    }
  : {})
```

In production, we still want to block certain paths like API endpoints that shouldn't be crawled by search engines.

#### Automatic Sitemap Integration

```typescript
sitemap: [origin + href("/sitemap.xml")],
```

This automatically includes your sitemap URL, using:

- `origin` from the request to get the correct domain
- `href(){:ts}` function for type-safe route references

<Alert type="tip">
  If you want to learn how to generate a sitemap in React Router 7, check out my
  [sitemap generation guide](/blog/sitemap-react-router-7).
</Alert>

## Register the route

Add the `robots.txt` route to your route configuration:

```typescript filename="app/routes.ts"
export default [
  // ...
  route("robots.txt", "routes/robots.txt.ts"),
] satisfies RouteConfig;
```

## Test the robots.txt output in the browser

Visit `/robots.txt` in your browser to see the generated file. You should see different output based on your environment:

**Production environment:**

```plaintext
User-agent: *
Allow: /
Disallow: /api/
Sitemap: https://yourdomain.com/sitemap.xml
```

**Non-production environment:**

```plaintext
User-agent: *
Disallow: /
Sitemap: http://localhost:5173/sitemap.xml
```

## Add dynamic logic or custom rules (optional)

You can extend the robots.txt with more sophisticated rules:

```typescript
const robotsTxt = generateRobotsTxt([
  {
    userAgent: "*",
    [isProductionDeployment ? "allow" : "disallow"]: ["/"],
    ...(isProductionDeployment
      ? {
          disallow: [
            "/api/",
            "/admin/",
            "/private/",
            "/*.json$", // Block JSON files
            "/search?*", // Block search result pages
          ],
          crawlDelay: 1, // Be nice to servers
        }
      : {}),
    sitemap: [origin + href("/sitemap.xml")],
  },
  // Specific rules for different bots
  ...(isProductionDeployment
    ? [
        {
          userAgent: "Googlebot",
          allow: ["/api/og/*"], // Allow OG image generation
        },
      ]
    : []),
]);
```

## Validate your robots.txt

Use these tools to validate your robots.txt file:

- [Google's robots.txt Tester](https://support.google.com/webmasters/answer/6062598)
- [Bing Webmaster Tools](https://www.bing.com/webmasters/)
- [robots.txt Checker](https://technicalseo.com/tools/robots-txt/)

For more advanced robots.txt configurations and SEO tools, check out the [official documentation](https://github.com/forge-42/seo-tools).
